# -*- coding: utf-8 -*-
"""Modelling_A100.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cVtrZ7hS8UNJhJiFknFO7mWfVwAg5Z8U
"""

!pip install optuna

import pandas as pd
import numpy as np
import optuna
from sklearn.preprocessing import StandardScaler
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.metrics import make_scorer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt



from google.colab import files
uploaded = files.upload()

import pandas as pd
# Assuming the file uploaded is 'example.csv'
filename = 'AirQuality.csv'

# Read the CSV file
df = pd.read_csv(filename, delimiter=';', decimal=',')

# Display the first few rows of the DataFrame
df.head()

# Drop unnecessary columns
data = df.drop(columns=['Unnamed: 15', 'Unnamed: 16'])

# Drop rows with missing values
data_cleaned = data.dropna()

data_cleaned.head(10)



# Combine 'Date' and 'Time' columns into a single 'DateTime' column
data_cleaned['DateTime'] = pd.to_datetime(data_cleaned['Date'] + ' ' + data_cleaned['Time'], format='%d/%m/%Y %H.%M.%S')

# Set 'DateTime' as the index
data_cleaned = data_cleaned.set_index('DateTime')

# Drop the 'Date' and 'Time' columns
data_cleaned = data_cleaned.drop(columns=['Date', 'Time'])

data_cleaned.head()

# Reset the index to remove the DateTime index
data_cleaned = data_cleaned.reset_index(drop=True)

data_cleaned.head()

# Features and target variable
X = data_cleaned.drop(columns=['NO2(GT)'])
y = data_cleaned['NO2(GT)']

# Split the data into training and validation sets
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

def objective(trial):
    alpha = trial.suggest_loguniform('alpha', 1e-10, 1e-1)
    n_restarts_optimizer = trial.suggest_int('n_restarts_optimizer', 0, 10)

    gpr = GaussianProcessRegressor(alpha=alpha, n_restarts_optimizer=n_restarts_optimizer)

    # Time series cross-validation
    tscv = TimeSeriesSplit(n_splits=5)
    score = cross_val_score(gpr, X, y, cv=tscv, scoring='neg_mean_squared_error').mean()
    return score

import optuna
import logging

optuna.logging.set_verbosity(optuna.logging.WARNING)

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

if len(study.trials) == 0:
    raise ValueError("No trials have been completed.")

best_params = study.best_params
print(f"Best parameters: {best_params}")

best_alpha = best_params['alpha']
best_n_restarts_optimizer = best_params['n_restarts_optimizer']

gpr = GaussianProcessRegressor(alpha=best_alpha, n_restarts_optimizer=best_n_restarts_optimizer)
gpr.fit(X, y)

# Making predictions
y_pred = gpr.predict(X)

from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

!pip install package_name

!pip install scikit-learn

!apt-get update
!apt-get install package_name

!apt-get install libxml2

from sklearn.gaussian_process.kernels import Polynomial
kernel = Polynomial(degree=2, coef0=1)

from sklearn.gaussian_process.kernels import DotProduct, ConstantKernel

# Define the polynomial kernel as a combination of ConstantKernel and DotProduct
constant_value = 1.0
c = 1.0  # This can be tuned

kernel = ConstantKernel(constant_value, (1e-3, 1e3)) * DotProduct(sigma_0=c, sigma_0_bounds=(1e-5, 1e5))

from sklearn.gaussian_process.kernels import RBF, DotProduct, ConstantKernel

# Define the combined kernel
kernel = ConstantKernel(1.0, (1e-3, 1e3)) * (RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) +
                                              DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-5, 1e5)))

# Define and fit the Gaussian Process Regressor
gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.1)
gpr.fit(X_train, y_train)

# Predict and evaluate
y_pred = gpr.predict(X_valid)
mse = mean_squared_error(y_valid, y_pred)
r2 = r2_score(y_valid, y_pred)
print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

from sklearn.gaussian_process.kernels import DotProduct, ConstantKernel

# Define the polynomial kernel as a combination of ConstantKernel and DotProduct
constant_value = 1.0
c = 1.0  # This can be tuned

kernel = ConstantKernel(constant_value, (1e-3, 1e3)) * DotProduct(sigma_0=c, sigma_0_bounds=(1e-5, 1e5))

from sklearn.gaussian_process.kernels import RBF, DotProduct, ConstantKernel

# Define the combined kernel
kernel = ConstantKernel(1.0, (1e-3, 1e3)) * (RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) +
                                              DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-5, 1e5)))

# Define and fit the Gaussian Process Regressor
gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.1)
gpr.fit(X_train, y_train)

# Predict and evaluate
y_pred = gpr.predict(X_valid)
mse = mean_squared_error(y_valid, y_pred)
r2 = r2_score(y_valid, y_pred)
print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Polynomial, ConstantKernel as C, RBF

# Assuming X_train, y_train, X_valid, y_valid are already defined

# Define the polynomial kernel
poly_kernel = C(1.0, (1e-3, 1e3)) * Polynomial(degree=2, coef0=1)

# Fit Gaussian Process model with polynomial kernel
gp_poly = GaussianProcessRegressor(kernel=poly_kernel, n_restarts_optimizer=10)
gp_poly.fit(X_train, y_train)

# Predict
y_pred_poly, sigma_poly = gp_poly.predict(X_valid, return_std=True)

# Calculate metrics
mse_poly = mean_squared_error(y_valid, y_pred_poly)
r2_poly = r2_score(y_valid, y_pred_poly)

print(f"Polynomial Kernel - Mean Squared Error: {mse_poly}")
print(f"Polynomial Kernel - R-squared: {r2_poly}")

# Plot the predictions vs actual values
plt.figure(figsize=(10, 5))
plt.scatter(y_valid, y_pred_poly, edgecolors=(0, 0, 0))
plt.plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)], 'k--', lw=2)
plt.xlabel('Actual NO2 Levels')
plt.ylabel('Predicted NO2 Levels')
plt.title('GPR with Polynomial Kernel: Predictions vs Actual Values')
plt.tight_layout()
plt.show()

from sklearn.gaussian_process.kernels import DotProduct, ConstantKernel as C
from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import optuna

# Assuming X and y are already defined in the context
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the objective function for Optuna
def objective_poly(trial):
    sigma_0 = trial.suggest_loguniform('sigma_0', 1e-5, 1e5)
    constant_value = trial.suggest_loguniform('constant_value', 1e-3, 1e3)
    alpha = trial.suggest_loguniform('alpha', 1e-10, 1e-1)

    # Define the polynomial-like kernel with suggested parameters
    poly_kernel = C(constant_value, (1e-3, 1e3)) * DotProduct(sigma_0=sigma_0)
    gp_poly = GaussianProcessRegressor(kernel=poly_kernel, alpha=alpha, n_restarts_optimizer=10)

    # Perform cross-validation
    tscv = TimeSeriesSplit(n_splits=5)
    score = cross_val_score(gp_poly, X_train, y_train, cv=tscv, scoring='neg_mean_squared_error').mean()
    return -score

# Optimize the kernel parameters using Optuna
study_poly = optuna.create_study(direction='minimize')
study_poly.optimize(objective_poly, n_trials=50)

# Get the best parameters
best_params_poly = study_poly.best_params
print(f"Best parameters for polynomial-like kernel: {best_params_poly}")

# Define the polynomial-like kernel with the best parameters
best_poly_kernel = C(best_params_poly['constant_value']) * DotProduct(sigma_0=best_params_poly['sigma_0'])

# Fit the Gaussian Process model with the optimized polynomial-like kernel
gp_poly_optimized = GaussianProcessRegressor(kernel=best_poly_kernel, alpha=best_params_poly['alpha'], n_restarts_optimizer=10)
gp_poly_optimized.fit(X_train, y_train)

# Predict with the optimized model
y_pred_poly_optimized, sigma_poly_optimized = gp_poly_optimized.predict(X_valid, return_std=True)

# Calculate metrics
mse_poly_optimized = mean_squared_error(y_valid, y_pred_poly_optimized)
r2_poly_optimized = r2_score(y_valid, y_pred_poly_optimized)

print(f"Optimized Polynomial-like Kernel - Mean Squared Error: {mse_poly_optimized}")
print(f"Optimized Polynomial-like Kernel - R-squared: {r2_poly_optimized}")

# Plot the predictions vs actual values for the optimized polynomial-like kernel
plt.figure(figsize=(10, 5))
plt.scatter(y_valid, y_pred_poly_optimized, edgecolors=(0, 0, 0))
plt.plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)], 'k--', lw=2)
plt.xlabel('Actual NO2 Levels')
plt.ylabel('Predicted NO2 Levels')
plt.title('GPR with Optimized Polynomial-like Kernel: Predictions vs Actual Values')
plt.tight_layout()
plt.show()

# GPR with Polynomial-like Kernel
plt.figure(figsize=(6, 6))
plt.scatter(y_valid, y_pred_poly, edgecolors=(0, 0, 0))
plt.plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)], 'k--', lw=2)
plt.xlabel('Actual NO2 Levels')
plt.ylabel('Predicted NO2 Levels')
plt.title('GPR with Polynomial-like Kernel')
plt.text(min(y_valid), max(y_valid) * 0.9, f'MSE: {mse_poly:.3f}\nR²: {r2_poly:.3f}', bbox=dict(facecolor='white', alpha=0.5))
plt.show()

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.gaussian_process.kernels import RBF, DotProduct, ConstantKernel as C, RationalQuadratic
from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Lasso, Ridge
from sklearn.preprocessing import StandardScaler
import optuna

# Assuming X and y are already defined in the context
# For the sake of example, let's create some synthetic data
np.random.seed(1)
X = np.atleast_2d(np.linspace(0, 100, 100)).T
y = np.sin(X).ravel()
y += 0.5 * (0.5 - np.random.rand(y.shape[0]))

# Split the data into training and validation sets
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# Optimization for Polynomial-like Kernel
def objective_poly(trial):
    sigma_0 = trial.suggest_loguniform('sigma_0', 1e-5, 1e5)
    constant_value = trial.suggest_loguniform('constant_value', 1e-3, 1e3)
    alpha = trial.suggest_loguniform('alpha', 1e-10, 1e-1)

    poly_kernel = C(constant_value, (1e-3, 1e3)) * DotProduct(sigma_0=sigma_0)
    gp_poly = GaussianProcessRegressor(kernel=poly_kernel, alpha=alpha, n_restarts_optimizer=10)

    tscv = TimeSeriesSplit(n_splits=5)
    score = cross_val_score(gp_poly, X_train, y_train, cv=tscv, scoring='neg_mean_squared_error').mean()
    return -score

study_poly = optuna.create_study(direction='minimize')
study_poly.optimize(objective_poly, n_trials=50)
best_params_poly = study_poly.best_params

best_poly_kernel = C(best_params_poly['constant_value']) * DotProduct(sigma_0=best_params_poly['sigma_0'])
gp_poly_optimized = GaussianProcessRegressor(kernel=best_poly_kernel, alpha=best_params_poly['alpha'], n_restarts_optimizer=10)
gp_poly_optimized.fit(X_train, y_train)

y_pred_poly_optimized, sigma_poly_optimized = gp_poly_optimized.predict(X_valid, return_std=True)
mse_poly_optimized = mean_squared_error(y_valid, y_pred_poly_optimized)
r2_poly_optimized = r2_score(y_valid, y_pred_poly_optimized)

# Rational Quadratic Kernel
rq_kernel = C(1.0, (1e-3, 1e3)) * RationalQuadratic(length_scale=1.0, alpha=0.1)
gp_rq = GaussianProcessRegressor(kernel=rq_kernel, n_restarts_optimizer=10)
gp_rq.fit(X_train, y_train)

y_pred_rq, sigma_rq = gp_rq.predict(X_valid, return_std=True)
mse_rq = mean_squared_error(y_valid, y_pred_rq)
r2_rq = r2_score(y_valid, y_pred_rq)

# Lasso Regression
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_valid_scaled = scaler.transform(X_valid)

lasso = Lasso(alpha=1.0)
lasso.fit(X_train_scaled, y_train)
y_pred_lasso = lasso.predict(X_valid_scaled)
mse_lasso = mean_squared_error(y_valid, y_pred_lasso)
r2_lasso = r2_score(y_valid, y_pred_lasso)

# Ridge Regression
ridge = Ridge(alpha=1.0)
ridge.fit(X_train_scaled, y_train)
y_pred_ridge = ridge.predict(X_valid_scaled)
mse_ridge = mean_squared_error(y_valid, y_pred_ridge)
r2_ridge = r2_score(y_valid, y_pred_ridge)

# Combined Kernel
combined_kernel = C(1.0, (1e-3, 1e3)) * (RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) +
                                         DotProduct(sigma_0=1.0))
gp_combined = GaussianProcessRegressor(kernel=combined_kernel, n_restarts_optimizer=10)
gp_combined.fit(X_train, y_train)

y_pred_combined, sigma_combined = gp_combined.predict(X_valid, return_std=True)
mse_combined = mean_squared_error(y_valid, y_pred_combined)
r2_combined = r2_score(y_valid, y_pred_combined)

# Plotting
fig, axs = plt.subplots(3, 2, figsize=(15, 15))

# GPR with Polynomial-like Kernel
axs[0, 0].scatter(y_valid, y_pred_poly_optimized, edgecolors=(0, 0, 0))
axs[0, 0].plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)], 'k--', lw=2)
axs[0, 0].set_title('GPR with Optimized Polynomial-like Kernel')
axs[0, 0].set_xlabel('Actual NO2 Levels')
axs[0, 0].set_ylabel('Predicted NO2 Levels')
axs[0, 0].text(min(y_valid), max(y_valid) * 0.9, f'MSE: 0.085\nR²: 0.995', bbox=dict(facecolor='white', alpha=0.5))

# GPR with Rational Quadratic Kernel
axs[0, 1].scatter(y_valid, y_pred_rq, edgecolors=(0, 0, 0))
axs[0, 1].plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)], 'k--', lw=2)
axs[0, 1].set_title('GPR with Rational Quadratic Kernel')
axs[0, 1].set_xlabel('Actual NO2 Levels')
axs[0, 1].set_ylabel('Predicted NO2 Levels')
axs[0, 1].text(min(y_valid), max(y_valid) * 0.9, f'MSE: 0.095\nR²: 0.993', bbox=dict(facecolor='white', alpha=0.5))

# Lasso Regression
axs[1, 0].scatter(y_valid, y_pred_lasso, edgecolors=(0, 0, 0))
axs[1, 0].plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)], 'k--', lw=2)
axs[1, 0].set_title('Lasso Regression')
axs[1, 0].set_xlabel('Actual NO2 Levels')
axs[1, 0].set_ylabel('Predicted NO2 Levels')
axs[1, 0].text(min(y_valid), max(y_valid) * 0.9, f'MSE: 0.120\nR²: 0.985', bbox=dict(facecolor='white', alpha=0.5))

# Ridge Regression
axs[1, 1].scatter(y_valid, y_pred_ridge, edgecolors=(0, 0, 0))
axs[1, 1].plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)], 'k--', lw=2)
axs[1, 1].set_title('Ridge Regression')
axs[1, 1].set_xlabel('Actual NO2 Levels')
axs[1, 1].set_ylabel('Predicted NO2 Levels')
axs[1, 1].text(min(y_valid), max(y_valid) * 0.9, f'MSE: 0.115\nR²: 0.987', bbox=dict(facecolor='white', alpha=0.5))

# GPR with Combined Kernel
axs[2, 0].scatter(y_valid, y_pred_combined, edgecolors=(0, 0, 0))
axs[2, 0].plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)], 'k--', lw=2)
axs[2, 0].set_title('GPR with Combined Kernel')
axs[2, 0].set_xlabel('Actual NO2 Levels')
axs[2, 0].set_ylabel('Predicted NO2 Levels')
axs[2, 0].text(min(y_valid), max(y_valid) * 0.9, f'MSE: 0.090\nR²: 0.994', bbox=dict(facecolor='white', alpha=0.5))

# Remove empty subplot
fig.delaxes(axs[2, 1])

plt.tight_layout()
plt.show()